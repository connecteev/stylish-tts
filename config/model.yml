# Configuration for training on a high-resource (24GB+ VRAM) GPU.

multispeaker: false
n_mels: 80
sample_rate: 24000
n_fft: 2048
win_length: 1200
hop_length: 300
style_dim: 64
inter_dim: 128

text_aligner:
  hidden_dim: 256
  token_embedding_dim: 512

decoder:
  hidden_dim: 512
  residual_dim: 64

generator:
  type: 'ringformer'
  resblock_kernel_sizes: [ 3, 7, 11 ]
  upsample_rates: [ 4, 5 ]
  upsample_initial_channel: 512
  upsample_last_channel: 128
  resblock_dilation_sizes: [ [ 1, 3, 5 ], [ 1, 3, 5 ], [ 1, 3, 5 ] ]
  upsample_kernel_sizes: [ 8, 10 ]
  gen_istft_n_fft: 60
  gen_istft_hop_size: 15
  depth: 2

text_encoder:
  tokens: 178 # number of phoneme tokens
  hidden_dim: 128
  filter_channels: 512
  heads: 8
  layers: 8
  kernel_size: 3
  dropout: 0.2

style_encoder:
  layers: 2

mel_style_encoder:
  max_channels: 384
  skip_downsample: True

duration_predictor:
  n_layer: 4
  duration_classes: 16
  max_duration: 50
  dropout: 0.2
  last_dropout: 0.5

pitch_energy_predictor:
  inter_dim: 512
  dropout: 0.2

# speech language model config
slm:
  model: 'microsoft/wavlm-base-plus'
  sr: 16000 # sampling rate of SLM

symbol:
  pad: "$"
  punctuation: ";:,.!?¡¿—…\"()“” "
  letters: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
  letters_ipa: "ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁᵊǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ"
